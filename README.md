# YOLOv5_Study
此文档主要用于记录YOLOv5的学习过程。目前主要还是从TensorRT加速网络的角度入手。但目前发现，不把网络内部的结构吃透，根本没法搞清楚如何修改和优化。因此，考虑作为一个长期的课题，进行探讨和分析。

## 1. 关于历史版本的问题。

YOLOv5是YOLO系列目标检测网络的经典之作。有必要将该项目完全吃透，以提升后续修改和使用其他类似目标检测网络的能力。

目前找到的网上教程是2022年以及2023年的版本（主要是恩培教程，以C++和TensorRT为主的一个教程）。其中的一些export.patch文件已经不是当时的版本。需要通过github的历史版本寻找当时的版本。虽然当时的版本有很多bug在后续时间修复了，但目前最合适的思路仍然是先将当时的版本在本地复现，然后再探索目前版本与当时版本的区别。

当前版本的地址是：https://github.com/ultralytics/yolov5

历史版本的地址是：https://github.com/ultralytics/yolov5/tree/7cef03dddd6fba26fff6748ed1cfdd18208c193e

## 2. YOLO前处理的CUDA手写中遇到的一些问题汇总

### 1. 安装C++版本的OpenCV4遇到的问题

C++的OpenCV4安装，基本流程大致如下：
1. 使用Ubuntu自带的apt工具，安装build-essential等支撑OpenCV的基本lib。
2. 下载OpenCV源码和contrib源码，contrib是一些非官方的代码，一些特殊的库可能会用到，最好安装。
3. 设计CMake的语句，设置需要安装和不需要安装的一些包，例如CUDA/CUDNN等需要显式规定出来。
4. 创建build文件夹，执行cmake指令。
5. 执行make指令，可以使用nproc观看CPU内核数量，再使用-j20等参数加快编译速度。
6. sudo make install，可以将OpenCV安装到系统的目录中，使得后续的CMake工具可以发现其目录。

过程中，遇到过的最头疼的问题是，安装后使用过程中，对程序进行make操作，出现了下面的报错：
/lib/x86_64-linux-gnu/libgio-2.0.so.0: undefined symbol: g_log_set_debug_enabled
这是由于安装C++版本的OpenCV时，没有关闭Conda环境导致的。切换到base环境，安装后依然报错。之后我直接卸载Conda，再安装OpenCV，亲测确实解决了问题，但也很伤，之前费了很大力气安装的环境都没了。
正确的操作，似乎应该是把conda关掉，命令行界面连(base)都不显示的那种状态，可惜没有去试。下次可以用公司服务器试试。

如果怕出现这种问题，还有一种方式可以规避，就是使用docker容器与宿主机环境隔离。在后续的**安装部署环境遇到的问题**章节有介绍。

### 2. CUDA核函数执行时间非常长

使用CUDA和C++编写YOLO的前处理程序，做HWC->CHW+normalize+BGR->RGB的操作，自编函数耗时长达100ms以上，而教程仅需要3ms。
经过各方面排查，包括访存方式等，最终发现，问题出在访问GPU内部数组越界上。正常的线程会一直等待越界的线程，直到超时退出。但此类问题，在GPU编程中并不会有明显的报错。
因此，这种情况需要仔细手动校验。这个也算是一个经验。

## 3. 安装部署环境遇到的问题 2025.06.06更新

### 1. docker环境

裸机直接安装环境非常不划算，合理使用docker是最合理的方案。而且docker可以保证一些关键的包都在正确的位置，更容易复现例程。

对于Yolov5，教程给的思路是把训练和部署分别放在两个docker环境里面，彼此不干扰。训练环境使用docker的另一个好处是，主机裸机可能有版本较高的cuda和torch等包版本，不一定对年代较早(2023年)的程序兼容性好。

对于x86架构的主机，直接拉取教程中建议的TensorRT docker环境，就能保证直接可以用。但这个环境是没有opencv的，如果想运行图像检测的runtime例程，还是需要再安装一次C++版本的opencv的。

对于arm64的主机，例如Jetson Orin AGX，由于我把jetpack更新到了6.2版本，所以TensorRT版本达到了10.0版本，非常高，直接裸机运行例程的cmake指令会报错，说tensorrt.cmake相关的文件有问题，应该是TensorRT版本不是8.x导致的。由于卸载再重装TensorRT比较麻烦，且很难保证是与Jetpack兼容的，因此我考虑使用更低版本的jetson专用的TensorRT镜像。可以从Nvidia官网寻找专用于arm64的TensorRT镜像，然后再在容器里安装opencv。

需要注意的是，Jetson Orin运行docker run会出现一些与iptable相关的报错，主要是因为其对容器桥接网络的默认配置支持不好。这种情况下，可以通过直接让容器使用宿主机的网络的方式实现，在docker run指令中加入"--network host"条件，即可让容器正常启动。

另外，对于宿主机上被容器挂载过的地址，如果有**通过容器操作**形成的文件，例如git拉取的文件或者创建的文件，在宿主机观看文件时都会显示无权限查看。如果强行给权限并查看，会导致被查看的文件git出现版本问题，例如不能再通过export.patch对export.py进行修改等问题。因此不要随意在宿主机内操作容器挂载的文件夹。

### 2. C++版本OpenCV安装

Linux环境下的C++OpenCV安装具有非常大的自由度，可选的东西非常多，这也是经过多次安装和卸载之后才明白的。

安装的大致顺序是：
1. 安装图像处理相关基础库
2. 下载源码
3. 配置cmake指令
4. build文件夹内运行cmake ..
5. build文件夹内运行make -jn
6. build文件夹内运行sudo make install

下面说一下每个步骤的注意事项
1. 安装图像处理相关基础库
OpenCV是个非常庞大的库，里面有很多个模块，而且很多都是可选装的。在后面第4步骤的cmake时，系统会自动检测计算机内是否安装了对应模块所需的支持库，如果没有这个支持库，cmake则会跳过这个模块的构建。
例如，如果不使用apt install安装ffmpeg相关的lib，则cmake会跳过视频流解析相关模块的构建，等后续想调用opencv去读取视频流时就会发现缺少相关的库，这就很坑。类似的还有gtk等，具体的可以多上网查查看。
所以，一定保证先把这些库安装好再去cmake，否则安装完发现缺少某些模块影响使用，还得卸载重装，非常麻烦。

2. 下载源码
opencv4.x有源码包可以从OpenCV官网下载。此外还有配套的opencv4.x-contrib可以在github下载，里面主要是一些官方不支持的，由个人或组织自己构建的一些扩展库，有时候还有有些用的，例如一些特征匹配相关的工具就在这个扩展包里面。

3. 配置cmake指令
cmake指令可以决定是否安装OpenCV的扩展包(contrib)支持、CUDA支持、python支持、DNN支持等。网上有很多种写法，具体可以根据自己的需求来确定。Jetpack自带的opencv，就是没有CUDA支持的，貌似是为了减少体积，只保留了基本功能。

4. build文件夹内运行cmake ..
没有特殊需要注意的。

5. build文件夹内运行make -jn
虽然直接make也可以编译opencv，但速度是没有多核编译快的。
可以在终端输入nproc指令，观看计算机有几个内核，然后用大部分核去编译。例如台式机有32个核，可以选26个核去编译，make -j26。Jetson Orin只有12个核，就使用make -j8编译。
这里可以看到Jetson Orin的编译速度远远慢于台式机，因为他的核少，且频率太低。

6. build文件夹内运行sudo make install
make install会把编译好的各种头文件和链接库放进系统的文件夹/usr/include等位置，因此是需要管理员权限的，所以加了sudo。在容器中直接有管理员权限，因此不需要加sudo。
需要注意的是，安装完成后不要急着删除build文件夹，因为如果想卸载opencv，可以在这个build文件夹里运行"sudo make uninstall"直接卸载OpenCV，比直接手动删除各种已安装的lib更方便，主要是不容易误删。

